{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Fast Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "To download stanford large movie review dataset, use this command:\n",
    "```sh\n",
    "wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "tar -xvf aclImdb_v1.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "max_time = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import collections\n",
    "\n",
    "train_path = 'aclImdb/train'\n",
    "data_tr_pos = []\n",
    "for f_name in listdir(train_path + '/pos'):\n",
    "    with open(train_path + '/pos/' + f_name) as f:\n",
    "        data_tr_pos.append(normalizeString(f.readlines()[0]).split()[:max_time])\n",
    "\n",
    "        data_tr_neg = []\n",
    "for f_name in listdir(train_path + '/neg'):\n",
    "    with open(train_path + '/neg/' + f_name) as f:\n",
    "        data_tr_neg.append(normalizeString(f.readlines()[0]).split()[:max_time])\n",
    "        \n",
    "test_path = 'aclImdb/test'\n",
    "data_tst_pos = []\n",
    "for f_name in listdir(test_path + '/pos'):\n",
    "    with open(test_path + '/pos/' + f_name) as f:\n",
    "        data_tst_pos.append(normalizeString(f.readlines()[0]).split()[:max_time])\n",
    "\n",
    "data_tst_neg = []\n",
    "for f_name in listdir(test_path + '/neg'):\n",
    "    with open(test_path + '/neg/' + f_name) as f:\n",
    "        data_tst_neg.append(normalizeString(f.readlines()[0]).split()[:max_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "for d in [data_tr_pos, data_tr_neg, data_tst_pos, data_tst_neg]:\n",
    "    for s in d:\n",
    "        for w in s:\n",
    "            words.append(w)\n",
    "            \n",
    "count = collections.Counter(words).most_common()\n",
    "\n",
    "dictionary = dict()\n",
    "dictionary['<Nothing>'] = 0\n",
    "for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\n",
    "del(words)\n",
    "del(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# data\n",
    "data_tr_concat = np.concatenate([data_tr_pos, data_tr_neg], 0)\n",
    "data_tst_concat = np.concatenate([data_tst_pos, data_tst_neg], 0)\n",
    "\n",
    "data_tr = []\n",
    "for i in range(len(data_tr_concat)):\n",
    "    tmp = [dictionary[x] for x in data_tr_concat[i]]\n",
    "    data_tr.append(tmp + [0]*(max_time-len(data_tr_concat[i])))\n",
    "data_tr_len = [len(x) for x in data_tr_concat]\n",
    "\n",
    "data_tst = []\n",
    "for i in range(len(data_tst_concat)):\n",
    "    tmp = [dictionary[x] for x in data_tst_concat[i]]\n",
    "    data_tst.append(tmp + [0]*(max_time-len(data_tst_concat[i])))\n",
    "data_tst_len = [len(x) for x in data_tst_concat]\n",
    "\n",
    "# labels\n",
    "label_tr = [1] * len(data_tr_pos) + [0] * len(data_tr_neg)\n",
    "label_tst = [1] * len(data_tst_pos) + [0] * len(data_tst_neg)\n",
    "\n",
    "del(data_tr_pos)\n",
    "del(data_tr_neg)\n",
    "del(data_tst_pos)\n",
    "del(data_tst_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(x, x_len, y, batch_size=128, shuffle=True):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    x_len = np.array(x_len)\n",
    "    \n",
    "    ids = np.arange(len(x))\n",
    "    if shuffle:\n",
    "        ids = np.random.permutation(ids)\n",
    "    \n",
    "    batch_num = int(np.ceil(len(ids) // batch_size))\n",
    "    \n",
    "    for i in range(batch_num):\n",
    "        idx_str = i * batch_size\n",
    "        idx_end = (i+1) * batch_size\n",
    "        yield x[ids[idx_str:idx_end]], x_len[ids[idx_str:idx_end]], y[ids[idx_str:idx_end]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(dictionary)\n",
    "hidden_dim = 10\n",
    "\n",
    "A = tf.Variable(np.random.rand(vocab_size, hidden_dim), dtype=tf.float32, name='lookup_table')\n",
    "B = tf.Variable(np.random.rand(hidden_dim, 1), dtype=tf.float32, name='output')\n",
    "\n",
    "stn = tf.placeholder(tf.int32, [None, max_time], 'stn')\n",
    "stn_len = tf.placeholder(tf.float32, [None], 'stn_len')\n",
    "label = tf.placeholder(tf.float32, [None], 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stn_emb = tf.nn.embedding_lookup(A, stn)\n",
    "hidden = tf.reduce_sum(stn_emb, 1) / tf.reshape(stn_len, [-1,1])\n",
    "logit = tf.matmul(hidden, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=tf.reshape(label, [-1,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Test & Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.float32, ())\n",
    "train = tf.train.GradientDescentOptimizer(a).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# logging\n",
    "tf.summary.scalar('learning_rate', a)\n",
    "tf.summary.scalar('cost', loss)\n",
    "summaries = tf.summary.merge_all()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epochs = 3000\n",
    "cnt = 0\n",
    "for epoch in range(epochs):\n",
    "    if epoch < 1000:\n",
    "        lrn_rate = .1\n",
    "    elif epoch < 1500:\n",
    "        lrn_rate = .03\n",
    "    elif epoch < 2000:\n",
    "        lrn_rate = .01\n",
    "    elif epoch < 2500:\n",
    "        lrn_rate = .003\n",
    "    else:\n",
    "        lrn_rate = .001\n",
    "        \n",
    "    batches = batch_iter(data_tr, data_tr_len, label_tr, batch_size=128)\n",
    "    \n",
    "    for x, x_len, y in batches:\n",
    "        _, s = sess.run([train, summaries], feed_dict={a: lrn_rate, stn:x, stn_len: x_len, label: y})\n",
    "    \n",
    "        summary_writer.add_summary(s, cnt)\n",
    "        cnt += 1\n",
    "    \n",
    "    # test\n",
    "    batches_tst = batch_iter(data_tst, data_tst_len, label_tst, batch_size=128, shuffle=False)\n",
    "    \n",
    "    rets = []\n",
    "    ys = []\n",
    "    for x, x_len, y in batches_tst:\n",
    "        ret = sess.run(logit, feed_dict={stn:x, stn_len: x_len, label: y})\n",
    "        rets.extend(ret)\n",
    "        ys.extend(y)\n",
    "    rets = np.array(rets).reshape(-1)\n",
    "    \n",
    "    precision = np.mean(np.round(1/(np.exp(-rets)+1)).reshape(-1) == ys)\n",
    "\n",
    "    precision_summ = tf.Summary()\n",
    "    precision_summ.value.add(tag='precision', simple_value=precision)\n",
    "    summary_writer.add_summary(precision_summ, epoch)\n",
    "    \n",
    "    print(epoch, end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Though this simple linear model only uses 10 hidden dimension, it achieves almost same results with RNN(http://domkaukinen.com/sentiment-analysis-with-tensorflow/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](logs/fasttext/precision.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf_r1.0_3.5]",
   "language": "python",
   "name": "conda-env-tf_r1.0_3.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
