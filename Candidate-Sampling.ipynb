{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Candidate-Sampling with Tensorflow\n",
    "\n",
    "We investigate some candidate sampling methods such as:\n",
    "* noise contrastive estimation\n",
    "* negative sampling\n",
    "* sampled softmax\n",
    "\n",
    "Additionally, we also implement hierarchical-softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Classification\n",
    "\n",
    "* minimized VGG model\n",
    "* cifar-100 dataset\n",
    "\n",
    "To download cifar-100 dataset, use this command:\n",
    "```sh\n",
    "curl -o cifar-100-binary.tar.gz https://www.cs.toronto.edu/~kriz/cifar-100-binary.tar.gz\n",
    "tar -xvf cifar-100-binary.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.1. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from cifar_input import build_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "images, labels = build_input('cifar100', './cifar-100-binary/train.bin', 100, 'train')\n",
    "drop_rate = tf.placeholder(tf.bool, ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def simpleCNN(X, rate, reuse=False):\n",
    "    X = tf.layers.conv2d(X, 64, [3, 3], padding='same', activation=tf.nn.relu, name='c11', reuse=reuse)\n",
    "    X = tf.layers.conv2d(X, 64, [3, 3], padding='same', activation=tf.nn.relu, name='c12', reuse=reuse)\n",
    "    X = tf.layers.max_pooling2d(X, [2, 2], [2, 2])\n",
    "    \n",
    "    X = tf.layers.conv2d(X, 128, [3, 3], padding='same', activation=tf.nn.relu, name='c21', reuse=reuse)\n",
    "    X = tf.layers.conv2d(X, 128, [3, 3], padding='same', activation=tf.nn.relu, name='c22', reuse=reuse)\n",
    "    X = tf.layers.max_pooling2d(X, [2, 2], [2, 2])\n",
    "    \n",
    "    X = tf.layers.conv2d(X, 256, [3, 3], padding='same', activation=tf.nn.relu, name='c31', reuse=reuse)\n",
    "    X = tf.layers.conv2d(X, 256, [3, 3], padding='same', activation=tf.nn.relu, name='c32', reuse=reuse)\n",
    "    X = tf.layers.conv2d(X, 256, [3, 3], padding='same', activation=tf.nn.relu, name='c33', reuse=reuse)\n",
    "    X = tf.layers.max_pooling2d(X, [2, 2], [2, 2])\n",
    "    \n",
    "    X = tf.layers.conv2d(X, 512, [3, 3], padding='same', activation=tf.nn.relu, name='c41', reuse=reuse)\n",
    "    X = tf.layers.conv2d(X, 512, [3, 3], padding='same', activation=tf.nn.relu, name='c42', reuse=reuse)\n",
    "    X = tf.layers.conv2d(X, 512, [3, 3], padding='same', activation=tf.nn.relu, name='c43', reuse=reuse)\n",
    "    X = tf.layers.max_pooling2d(X, [2, 2], [2, 2])\n",
    "    \n",
    "    X = tf.contrib.layers.flatten(X)\n",
    "    X = tf.layers.dense(X, 1048, activation=tf.nn.relu, name='d1', reuse=reuse)\n",
    "    X = tf.layers.dropout(X, rate)\n",
    "    X = tf.layers.dense(X, 1048, activation=tf.nn.relu, name='d2', reuse=reuse)\n",
    "    X = tf.layers.dropout(X, rate)\n",
    "    X = tf.layers.dense(X, 100, name='d3', reuse=reuse)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "logit = simpleCNN(images, drop_rate)\n",
    "\n",
    "images_tst, labels_tst = build_input('cifar100', './cifar-100-binary/test.bin', 100, 'test')\n",
    "logit_tst = simpleCNN(images_tst, drop_rate, reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.2. Training Type\n",
    "\n",
    "* softmax\n",
    "* sampled-softmax\n",
    "* hierarchical-softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 1.2.1. Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 1.2.2. Sampled-softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "l_weights = tf.Variable(tf.truncated_normal([100, 1048],\n",
    "                                              stddev=1.0 / np.sqrt(1048)))\n",
    "l_biases = tf.Variable(tf.zeros([100]))\n",
    "\n",
    "\n",
    "# loss automatically draws a new sample of the negative labels each\n",
    "# time we evaluate the loss.\n",
    "num_true = 1\n",
    "num_sampled = 30\n",
    "num_classes = 100\n",
    "\n",
    "sampled_values = tf.nn.uniform_candidate_sampler(\n",
    "          true_classes=tf.reshape(tf.arg_max(labels, 1), [-1,1]),\n",
    "          num_true=num_true,\n",
    "          num_sampled=num_sampled,\n",
    "          unique=True,\n",
    "          range_max=num_classes)\n",
    "\n",
    "cost = tf.reduce_mean(\n",
    "  tf.nn.sampled_softmax_loss(weights=l_weights,\n",
    "                 biases=l_biases,\n",
    "                 labels=tf.reshape(tf.arg_max(labels, 1), [-1,1]),\n",
    "                 inputs=logit.graph.get_operation_by_name('dropout_2/Identity').outputs[0],\n",
    "                 num_sampled=num_sampled,\n",
    "                 num_classes=num_classes,\n",
    "                 sampled_values=sampled_values))\n",
    "                 \n",
    "logit = tf.matmul(logit.graph.get_operation_by_name('dropout_2/Identity').outputs[0], tf.transpose(l_weights)) + l_biases\n",
    "logit_tst = tf.matmul(logit_tst.graph.get_operation_by_name('dropout_4/Identity').outputs[0], tf.transpose(l_weights)) + l_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 1.2.3. Hierarchical-softmax\n",
    "\n",
    "For convinience, we use two-layer hierarchical-softmax.\n",
    "\n",
    "Actually, two-layer approach is best for CIFAR-100, because all training/test labels are uniformly distributed.\n",
    "\n",
    "---\n",
    "**Note**\n",
    "\n",
    "* This Hierarchical-Softmax is not memory-efficient nor gpu-friendly.\n",
    "* That is the reason why tensorflow-version hierarchical-softmax is not implemented publicly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ids = tf.arg_max(labels, 1)\n",
    "ids_1st = tf.cast(ids // 10, tf.int32)\n",
    "ids_2nd = tf.cast(ids % 10, tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inputs = logit.graph.get_operation_by_name('dropout_2/Identity').outputs[0]\n",
    "hs_1st = tf.nn.softmax(tf.layers.dense(inputs, 10, name='hs_1st'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "p1 = tf.gather(tf.reshape(hs_1st, [-1,1]), ids_1st + tf.range(0, 1000, 10))\n",
    "\n",
    "p2 = []\n",
    "for i in range(10):\n",
    "    matched_nums = tf.reshape(tf.where(tf.equal(ids_1st, i)), [-1])\n",
    "    matched_ids_2nd = tf.gather(ids_2nd, matched_nums)\n",
    "    matched_inputs = tf.gather(inputs, matched_nums)\n",
    "    \n",
    "    matched_outputs = tf.nn.softmax(tf.layers.dense(matched_inputs, 10, name='hs_2nd_%i' % i))\n",
    "    matched_ps = tf.gather(tf.reshape(matched_outputs, [-1,1]), matched_ids_2nd + tf.range(0, tf.reduce_prod(tf.shape(matched_outputs)), 10))\n",
    "    p2.append(matched_ps)\n",
    "    \n",
    "p2 = tf.concat(p2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "p_t = p1 * p2\n",
    "cost = -tf.reduce_mean(tf.log(p_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "logit_1 = tf.nn.softmax(tf.layers.dense(inputs, 10, name='hs_1st', reuse=True))\n",
    "logit_2 = [tf.nn.softmax(tf.layers.dense(inputs, 10, name='hs_2nd_%i' % i, reuse=True)) for i in range(10)]\n",
    "logit_2_concat = tf.concat(logit_2, 1)\n",
    "\n",
    "logit_mul_shape = tf.concat([tf.shape(logit_1), [tf.shape(logit_2)[-1]]], 0)\n",
    "\n",
    "logit = tf.reshape(tf.expand_dims(logit_1, 2) * \n",
    "                   tf.reshape(logit_2_concat, logit_mul_shape), \n",
    "                   tf.shape(logit_2_concat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inputs_tst = logit.graph.get_operation_by_name('dropout_4/Identity').outputs[0]\n",
    "logit_tst_1 = tf.nn.softmax(tf.layers.dense(inputs_tst, 10, name='hs_1st', reuse=True))\n",
    "logit_tst_2 = [tf.nn.softmax(tf.layers.dense(inputs_tst, 10, name='hs_2nd_%i' % i, reuse=True)) for i in range(10)]\n",
    "logit_tst_2_concat = tf.concat(logit_tst_2, 1)\n",
    "\n",
    "logit_tst_mul_shape = tf.concat([tf.shape(logit_tst_1), [tf.shape(logit_tst_2)[-1]]], 0)\n",
    "\n",
    "logit_tst = tf.reshape(tf.expand_dims(logit_tst_1, 2) * \n",
    "                   tf.reshape(logit_tst_2_concat, logit_tst_mul_shape), \n",
    "                   tf.shape(logit_tst_2_concat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.3. Training & Test & Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.float32, ())\n",
    "optimizer = tf.train.GradientDescentOptimizer(a)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "tf.train.start_queue_runners(sess)\n",
    "sess.run(init)\n",
    "\n",
    "# logging\n",
    "tf.summary.scalar('learning_rate', a)\n",
    "tf.summary.scalar('cost', cost)\n",
    "summaries = tf.summary.merge_all()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "training_epochs = 50000\n",
    "display_step = 500\n",
    "\n",
    "for epoch in range(training_epochs+1):\n",
    "    if epoch < 20000:\n",
    "        lrn_rate = 0.1\n",
    "    elif epoch < 30000:\n",
    "        lrn_rate = 0.01\n",
    "    elif epoch < 40000:\n",
    "        lrn_rate = 0.001\n",
    "    else:\n",
    "        lrn_rate = 0.0001\n",
    "        \n",
    "    _, s, c = sess.run([train, summaries, cost], feed_dict={a: lrn_rate, drop_rate: 0.1})\n",
    "    summary_writer.add_summary(s, epoch)\n",
    "    \n",
    "    print (\"Epoch:\", '%04d' %(epoch), \"cost:\", \"{:0.9f}\".format(c), end='\\r')\n",
    "\n",
    "    if epoch % display_step == 0:\n",
    "        print()\n",
    "        rets, anss = [], []\n",
    "        for i in range(100):\n",
    "            ret, ans = sess.run([logit_tst, labels_tst], feed_dict={drop_rate: 0.0})\n",
    "            rets.extend(ret)\n",
    "            anss.extend(ans)\n",
    "        precision = np.mean(np.argmax(rets, 1) == np.argmax(anss, 1)) \n",
    "        print(precision)\n",
    "        \n",
    "        precision_summ = tf.Summary()\n",
    "        precision_summ.value.add(tag='precision', simple_value=precision)\n",
    "        summary_writer.add_summary(precision_summ, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![title](logs/candidate-sampling/precision.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Word2Vec\n",
    "\n",
    "* minimized VGG model\n",
    "* cifar-100 dataset\n",
    "\n",
    "To download cifar-100 dataset, use this command:\n",
    "```sh\n",
    "curl -o cifar-100-binary.tar.gz https://www.cs.toronto.edu/~kriz/cifar-100-binary.tar.gz\n",
    "tar -xvf cifar-100-binary.tar.gz\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf_r1.0_3.5]",
   "language": "python",
   "name": "conda-env-tf_r1.0_3.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
